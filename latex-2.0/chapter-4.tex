% -*- coding: utf-8; -*-
\chapter{Computational Experiments and Analysis}
\label{chap:experiments}
In this chapter, we discuss the computational experiments and the analysis that emerge from them. From now on, all results regarding the proposed algorithm will be identified as HG-means (for Hybrid Genetic K-Means). The results are presented in terms of objective function value and time, and are compared to current state-of-the-art results in the MSSC literature.

The experiments were conducted on an Intel Core i5 2.6 GHz processor machine with 8 GB of RAM memory. The source codes were written in C++, using the UNIX g++ compiler on a Linux Ubuntu 14.04 LTS 64-bit operating system.

Three different analysis are presented in this chapter. The first one analyses the general performance of the proposed algorithm in terms of objective function value and computational time, where we compare our results with the current literature. The second one analyses the impact of instances characteristics, where the objective is to verify if the proposed algorithm is affected by the number of clusters and the size of instances. Finally, we analyse the contribution of crossover and mutation components in the performance of the method.

In the coming result tables, the following notation is used:

\begin{itemize}
	\item $n$ is the size of the instance (number of data points);
	
	\item $m$ is the number of clusters;

	\item $d$ is the number of features;
	
	\item $f_{best}$ is the best known value for the MSSC objective found so far;

	\item $E$ is the error from the best known solution, calculated as:

		\begin{center}
		\large
			$E = \frac{f - f_{best}}{f_{best}} \times 100$
		\end{center}
		
	where $f$ is the value of the MSSC objective found by an algorithm. For HG-means, $E_{med}$ and $E_{avg}$ report the error from the median and average values, respectively;
	
	\item $t$ is the CPU time in seconds;
	
	%\item Bold values are used when HG-means found the new best solution

\end{itemize}

\section{Instances}
\label{sec:instances}
The selection of instances was done in order to cover different types of real data, considering both the instance size and the number of features as important indicators to describe the nature of data. Table \ref{instances} summarizes the characteristics of the adopted instances, which correspond to 24 important benchmarks in recent clustering literature, as reported in \cite{Ordin2014} and \cite{Bagirov2016}. For all considered instances, every feature value is a real or integer number and there is no missing values. The number of data points (instances size) ranges from 59 (smallest) to 434,874 (largest); and the number of features ranges from 2 (smallest) to 128 (largest).

In order to elucidate the coming analysis, we propose the discrimination of these instances according to their sizes, that from now on will by identified as Group A1, Group A2, Group B and Group C of instances.

Groups A1 and A2 correspond to small instances reported in the work of \cite{Ordin2014}. The difference between them is that A1 is composed by really small instances with up to 150 data points, whereas instances in A2 have some hundreds of data points. This distinction is useful to choose a reasonable number of clusters to be tested in each group. For instances A1, tests are performed considering up to 10 clusters, whereas in A2 the number of clusters is extended. Group B of instances correspond to medium/large instances reported also by \cite{Ordin2014}, comprising data where the number of points ranges from 1,060 to 20,000. Group C of instances are the same reported in \cite{Bagirov2016}, where instance sizes ranges from 13,910 to 434,874 data points. This last group is especially relevant as it consider large real world instances, which allows a deeper analysis in the scaling of algorithms.

\input{tables/instances}

\section{Parameters calibration}
\label{sec:calibration}
As in most meta-heuristics, the values chosen for parameters directly affect the results. In this work, we consider five main parameters: $w$ (the tournament size for selection), $\mu$ (the population size), $\Pi$ (the maximum size of population), $I_{max}$ (the maximum number of iterations) and $I_S$ (the number of iterations without improvement that causes the algorithm stop). For the parameters related to the management of individuals in the population ($w$, $\mu$ and $\Pi$) we performed a calibration to choose the best configuration for the experiments. For the parameters related to the number of iterations the algorithm takes ($I_{max}$ and $I_S$), we directly set their values to 4000 and 2500, respectively. These values were set in order to obtain results in a time comparable to previous authors.

In preliminary experiments, we started with the following configuration for the free variables, as they produced good and stable results after an initial and manually exploration: $w$ = 3, $\mu$ = 80 and $\Pi$ = 300. From this baseline, we expanded the range of these values by an one-factor-at-a-time (OFAT) approach. Table \ref{calibration} presents the ranges of tested values for each parameter and the final values achieved after increasing/decreasing each parameter value at a time.

\input{tables/calibration}

We considered a subset of five instances for calibration, that were chosen based on their sizes and number of features (Liver disorders, Ionosphere, Breast cancer, Pima Indians diabetes and TSPLIB1060). Small instances were not considered because in almost all of them the different configurations for calibration achieved the best known result (possibly the global optimal), so they are not so informative. As we consider 3 parameters independently, the combination of parameters resulted in 11 possible configurations, as shown in Table \ref{calibration-results}. For this reason, large instances were also not considered.
%It would take too much time just to calibrate the algorithm, as we run 10 times each experiment.
We chose medium-sized instances and tested 4 different number of clusters (20, 30, 40, 50) for each instance. After measuring the offset between the average error and the execution time for each configuration, we took the best one among the 11 options.

In Table \ref{calibration-results}, the first three columns report the values of parameters $w$, $\mu$ and $\Pi$, whereas the remaining columns report, in turn, the errors from the best, worst, median and average solution, and finally the average time taken to run the considered instances for calibration.

\input{tables/calibration-results}

\section{Experimental results}
\label{sec:results}

\subsection{General performance and computational time}
\label{sec:performance}
The results of numerical experiments regarding solution performance and computational time for groups A1, A2, B and C of instances are presented in Tables \ref{results-all-A1}, \ref{results-all-A2}, \ref{results-all-B} and \ref{results-all-C}, respectively. Due to the stochastic nature of the proposed meta-heuristic, the results of HG-means correspond to the average of 10 runs, where a run is the execution of an instance with a specific $m$. 

For comparison purposes, we analyse the results considering the best known solutions found so far and the results of global K-means (GKM) \cite{Likas2003}, the modified global K-means (MGKM) \cite{Bagirov2008}, the multi-start modified global K-means (MS-MGKM) \cite{Ordin2014} and the difference of convex clustering (DCClust and MS-DCA) \cite{Bagirov2016} algorithms, which are recent works in MSSC literature, corresponding to the state-of-the-art for this problem. For Tables \ref{results-all-A1}, \ref{results-all-A2}, \ref{results-all-B}, the computational time of MS-MGKM, MGKM and GKM algorithms correspond to the measurements performed by an Intel Core 2 Dual 2.50 GHz processor machine with 3 GB of RAM memory, as reported in \cite{Ordin2014}. For Table \ref{results-all-C}, an Intel Core i5 2.90 GHz processor machine with 8 GB of RAM was used to measure DCClust and MS-DCA algorithms, according to \cite{Bagirov2016}.

% HG-means performance analysis
Tables \ref{results-all-A1} and \ref{results-all-A2} presents the results for instances of group A1 and A2. As we can observe, HG-means finds the new best solution or achieves the current best one in all instances for most of $m$ values. In some cases, as in Ionosphere instance, HG-means results reach more than 4\% of the error, a significant improvement. On the other hand, the proposed algorithm requires more computational effort than MS-MGKM, MGKM and GKM algorithms for most of these small instances -- in many cases the computational time of HG-means is similar to MS-MGKM. It is also important to note that in some instances like Bavaria postal 2, Liver disorders, Ionosphere and Pima Indians diabetes, the gap of HG-means results to the best known solutions increases for large $m$.

Table \ref{results-all-B} presents the results for instances of group B. As in results for small instances, HG-means finds the new best solution or achieves the current best one in all instances for most of $m$ values, outperforming MS-MGKM, MGKM and GKM algorithms in terms of solution quality. Regarding computational time, HG-means is faster than MS-MGKM and GKM in TSPLIB3038, faster than MS-MGKM in TSPLIB1060 and has nearly the same time of MS-MGKM and MGKM in Pendigit. For Image segmentation and Page Block -- the latter, only for large $m$ -- HG-means requires more computational effort.

Table \ref{results-all-C} presents the results for instances of group C -- the group with the largest instances. As in the previous results, HG-means finds the new best solution or achieves the current best one in all instances for most of $m$ values. Regarding computational time, HG-means is faster than DCClust for almost all instances, except for EEG eye state and D15112.

\input{tables/results-all-A1}

\input{tables/results-all-A2}

\input{tables/results-all-B}

\input{tables/results-all-C}

\subsection{Impact of the number of clusters}

From the performance observed in the results of HG-means algorithm in both solution quality and computational time, we sought to verify if the algorithm works well by increasing the number of clusters. %This analysis is very important, since clustering tasks that require more groups are, in general, more difficult tasks, since we are dealing with a larger combinatorial problem.
This analysis is very important to confirm the robustness of the method, since we are dealing with a larger combinatorial problem when $m$ is large.

Table \ref{m-analysis} presents how HG-means performs for different numbers of clusters. For each group of instances, it reports the average error ($E_{avg}$) to the best known solution when varying the number of clusters ($m$). For instances A1, A2 and B, HG-means has the most significant improvements -- compared to the best known solution -- in cases where $m$ is large. For instances of group C, HG-means increased the gap to the best known solution when $m$ = (2, 20, 25), confirming the robustness of the method.

\input{tables/m-analysis}

Figures \ref{fig:error-A1} to and \ref{fig:error-C} display the percentage gap of the methods when considering different number of clusters. In these figures, results are shown for each instance, rather than the aggregated values of Table \ref{m-analysis}.

%\begin{figure}[H]
%\centering
%\subfigure[TSPLIB1060]{\label{fig:errorB-a}\includegraphics[width=0.45\textwidth]{img/error-tsplib1060}}
%\subfigure[Image segmentation]{\label{fig:errorB-b}\includegraphics[width=0.45\textwidth]{img/error-image}}
%\subfigure[TSPLIB3038]{\label{fig:errorB-c}\includegraphics[width=0.45\textwidth]{img/error-tsplib3038}}
%\subfigure[Page blocks]{\label{fig:errorB-d}\includegraphics[width=0.45\textwidth]{img/error-page}}
%\subfigure[Pendigit]{\label{fig:errorB-e}\includegraphics[width=0.45\textwidth]{img/error-pendigit}}
%\subfigure[Letter]{\label{fig:errorB-f}\includegraphics[width=0.45\textwidth]{img/error-letter}}
%\caption{The algorithms error from the best known solutions vs the number of clusters (Instances B)}
%\label{fig:errorB}
%\end{figure}
%
%\begin{figure}[H]
%\centering
%\subfigure[Gas sensor]{\label{fig:errorC-a}\includegraphics[width=0.45\textwidth]{img/error-gas}}
%\subfigure[EEG eye state]{\label{fig:errorC-b}\includegraphics[width=0.45\textwidth]{img/error-eye}}
%\subfigure[D15112]{\label{fig:errorC-c}\includegraphics[width=0.45\textwidth]{img/error-d15112}}
%\subfigure[KEGG metabolic relation network]{\label{fig:errorC-d}\includegraphics[width=0.45\textwidth]{img/error-kegg}}
%\subfigure[Shuttle control]{\label{fig:errorC-e}\includegraphics[width=0.45\textwidth]{img/error-shuttle}}
%\subfigure[Pla85900]{\label{fig:errorC-f}\includegraphics[width=0.45\textwidth]{img/error-pla85900}}
%\subfigure[Skin segmentation]{\label{fig:errorC-g}\includegraphics[width=0.45\textwidth]{img/error-skin}}
%\subfigure[3D road network]{\label{fig:errorC-h}\includegraphics[width=0.45\textwidth]{img/error-3d}}
%\caption{The algorithms error from the best known solutions vs the number of clusters (Instances C)}
%\label{fig:errorC}
%\end{figure}

\begin{figure}[H]
\centering
\subfigure[$m$ = 2]{\label{fig:error-A1-2}\includegraphics[width=1\textwidth]{img/error-A1-2}}
\subfigure[$m$ = 5]{\label{fig:error-A1-5}\includegraphics[width=1\textwidth]{img/error-A1-5}}
\subfigure[$m$ = 10]{\label{fig:error-A1-10}\includegraphics[width=1\textwidth]{img/error-A1-10}}
\caption{Algorithms performance for different number of clusters (Instances A1)}
\label{fig:error-A1}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[$m$ = 10]{\label{fig:error-A2-10}\includegraphics[width=1\textwidth]{img/error-A2-10}}
\subfigure[$m$ = 30]{\label{fig:error-A2-30}\includegraphics[width=1\textwidth]{img/error-A2-30}}
\subfigure[$m$ = 50]{\label{fig:error-A2-50}\includegraphics[width=1\textwidth]{img/error-A2-50}}
\caption{Algorithms performance for different number of clusters (Instances A2)}
\label{fig:error-A2}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[$m$ = 20]{\label{fig:error-B-20}\includegraphics[width=1\textwidth]{img/error-B-20}}
\subfigure[$m$ = 40]{\label{fig:error-B-40}\includegraphics[width=1\textwidth]{img/error-B-40}}
\subfigure[$m$ = 80]{\label{fig:error-B-80}\includegraphics[width=1\textwidth]{img/error-B-80}}
\caption{Algorithms performance for different number of clusters (Instances B)}
\label{fig:error-B}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[$m$ = 15]{\label{fig:error-C-15}\includegraphics[width=1\textwidth]{img/error-C-15}}
\subfigure[$m$ = 20]{\label{fig:error-C-20}\includegraphics[width=1\textwidth]{img/error-C-20}}
\subfigure[$m$ = 25]{\label{fig:error-C-25}\includegraphics[width=1\textwidth]{img/error-C-25}}
\caption{Algorithms performance for different number of clusters (Instances C)}
\label{fig:error-C}
\end{figure}

\subsection{Impact of instances size}
Section \ref{sec:performance} shows that the objective value of solutions produced by HG-means outperformed the compared algorithms in most experiments. This section presents some analysis regarding the HG-means scaling, i.e., how much computational time does the algorithm require in large data. Figures \ref{fig:timeB} and \ref{fig:timeC} compare HG-means with MS-MGKM, MGKM, GKM and DCClust in terms of CPU time as a function of the number of clusters $m$. The elapsed time was measured in seconds and the results are presented for groups B and C of instances. For small instances, there is a very small difference in the computational time of the considered algorithms. Therefore, the results for groups A1 and A2 are not presented in this analysis.

%For instances of group A2, HG-means demands more computational efforts than MS-MGKM, MGKM, GKM and DCClust for all values of $m$.
For group B of instances, HG-means takes a similar amount of time when compared to other algorithms, and has its most competitive performance when $m$ is increased. For instances of group C, HG-means is faster than the other algorithms exactly in the largest instances (Pla85900, Skin segmentation and 3D road), having a similar computational time for the remaining instances in this group.

Therefore, HG-means is very promising regarding scalability, as it produces better solutions and requires less computational time than the compared algorithms on large instances. In addition, there is room to adjust the number of iterations the algorithm takes and to use more efficient data structures that can improve runtime.

%\begin{figure}[H]
%\centering
%\subfigure[$m$ = 2]{\label{fig:sizeB-2}\includegraphics[width=1\textwidth]{img/sizeB-2}}
%\subfigure[$m$ = 10]{\label{fig:sizeB-10}\includegraphics[width=1\textwidth]{img/sizeB-10}}
%\subfigure[$m$ = 20]{\label{fig:sizeB-20}\includegraphics[width=1\textwidth]{img/sizeB-20}}
%\caption{The CPU time of algorithms in instances of group B}
%\label{fig:sizeB}
%\end{figure}
%
%\begin{figure}[H]
%\centering
%\subfigure[$m$ = 2]{\label{fig:sizeC-2}\includegraphics[width=1\textwidth]{img/sizeC-2}}
%\subfigure[$m$ = 10]{\label{fig:sizeC-10}\includegraphics[width=1\textwidth]{img/sizeC-10}}
%\subfigure[$m$ = 20]{\label{fig:sizeC-20}\includegraphics[width=1\textwidth]{img/sizeC-20}}
%\caption{The CPU time of algorithms in instances of group C}
%\label{fig:sizeC}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[H]
\centering
\subfigure[TSPLIB1060 ($n$ = 1060)]{\label{fig:time1}\includegraphics[width=0.48\textwidth]{img/cpu-tsplib1060}}
\subfigure[Image segmentation ($n$ = 2310)]{\label{fig:time2}\includegraphics[width=0.48\textwidth]{img/cpu-image}}
\subfigure[TSPLIB3038 ($n$ = 3038)]{\label{fig:time3}\includegraphics[width=0.48\textwidth]{img/cpu-tsplib3038}}
\subfigure[Page blocks ($n$ = 5473)]{\label{fig:time4}\includegraphics[width=0.48\textwidth]{img/cpu-page}}
\subfigure[Pendigit ($n$ = 10992)]{\label{fig:time5}\includegraphics[width=0.48\textwidth]{img/cpu-pendigit}}
\subfigure[Letter ($n$ = 20000)]{\label{fig:time6}\includegraphics[width=0.48\textwidth]{img/cpu-letter}}
\caption{The CPU time of algorithms (Instances B)}
\label{fig:timeB}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[Gas sensor ($n$ = 13910)]{\label{fig:time7}\includegraphics[width=0.45\textwidth]{img/cpu-gas}}
\subfigure[EEG eye state ($n$ = 14980)]{\label{fig:time8}\includegraphics[width=0.45\textwidth]{img/cpu-eye}}
\subfigure[D15112 ($n$ = 15112)]{\label{fig:time9}\includegraphics[width=0.45\textwidth]{img/cpu-d15112}}
\subfigure[KEGG metabolic ($n$ = 53413)]{\label{fig:time10}\includegraphics[width=0.45\textwidth]{img/cpu-kegg}}
\subfigure[Shuttle control ($n$ = 58000)]{\label{fig:time11}\includegraphics[width=0.45\textwidth]{img/cpu-shuttle}}
\subfigure[Pla85900 ($n$ = 85900)]{\label{fig:time12}\includegraphics[width=0.45\textwidth]{img/cpu-pla85900}}
\subfigure[Skin segmentation ($n$ = 245057)]{\label{fig:time13}\includegraphics[width=0.45\textwidth]{img/cpu-skin}}
\subfigure[3D road network ($n$ = 434874)]{\label{fig:time14}\includegraphics[width=0.45\textwidth]{img/cpu-3droad}}
\caption{The CPU time of algorithms (Instances C)}
\label{fig:timeC}
\end{figure}

\subsection{Components}
\label{sec:components-section}
In order to understand the contribution of components in the performance of HG-means, we performed some experiments by removing two important operators at a time: crossover and mutation. For comparison purposes, Table \ref{comp} reports the average error with respect to the best known solution in three scenarios: (i) complete HG-means with all components ($E_{avg}$), (ii) HG-means without mutation ($E_{avg}$ -M) and (iii) HG-means without crossover ($E_{avg}$ -C).

\small{\input{tables/comp}}

Mutation and crossover are both essential to find high-quality solutions. For the case where we have HG-means without mutation, it does not find (in average) a solution better than the best known so far, while in the two other scenarios (complete HG-means and HG-means without crossover), it has a significantly better performance. Additionally, the complete HG-means algorithm performs better than the HG-means without crossover, especially when $m$ increases, indicating that the crossover is an essential operator when dealing with more complex clustering tasks.

%\noindent [TO-DO] analysis on number of features -- our algorithm seems to be fast when $d$ is small in small/medium instances (tsplib for example) but also when $d$ is large in large instances (gas sensor for example)