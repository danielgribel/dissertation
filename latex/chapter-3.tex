\chapter{Proposed Methodology}
\label{chap:methodology}
This chapter describes the proposed method to solve the MS-Mean problem, the most treated objective in data clustering. The designed method is based on a genetic algorithm (GA) with local improvements combined with mechanisms that allow the diversification (local minima escape) and the propagation of good solutions. In this method, the local improvement is done through the running of the k-means algorithm, which takes one candidate solution in the GA population as a starting point. In other words, the method may be defined as a multi-start k-means inside a GA framework, which in turn is guided by the MS-Mean objective.

\section{General structure}
The general scheme of the meta-heuristic here addressed is described by algorithm \ref{genetic-algo}. Initially, the method generates a random population of individuals. Then, it applies successively a number of operators to evolve this population. Firstly, it selects two parent individuals from the population and combines them by a crossover procedure, yielding to a new individual (offspring) that is added to the population. Secondly, the offspring is mutated and enhanced by a local search, generating a new individual solution that is also added to the population. These two steps -- crossover and mutation with enhancement -- are performed many times until a termination criteria is reached.

In addition to the genetic operators mentioned above, two mechanisms -- one to select survivor individuals and the other to diversify the population -- are applied when some criteria are reached. These mechanisms allow the method to keep and propagate the best individuals and to introduce some diversification, respectively. The following sub-sections describes how these operators and mechanisms to evolve and manage the population of solutions work.

\section{Solution representation}
A solution (clustering partition) is represented by a direct encoding of the object-cluster assignment. The idea is to use a genetic encoding that allocates directly $n$ objects to $k$ clusters, such that each candidate solution consists of a $n$-vector ($n$ genes) with integer values in the interval [1, $k$]. Thus, for $n$ = 4 and $k$ = 3, the encoding (1,1,3,2) allocates the first and the second objects to cluster 1; the third object to cluster 3 and the fourth object to cluster 2, generating the partition (\{1,2\}, \{3\}, \{4\}).

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{images/assignment-encoding}
    \caption{An example of the assignment encoding for $n$ = 4}\label{fig:assignment-encoding}
  \end{center}
\end{figure}

An encoding based on the centroids description is also considered in order to make the proposed crossover and mutation operations practicable. A centroids encoding defines a solution through the feature vectors existing in each centroid, being a matrix $C$ of size $k \times d$, where $k$ is the number of centroids (clusters) and $d$ is the number of features (see figure \ref{fig:centroids-encoding}). Thus, an entry $c_{ij}$ of $C$ corresponds to the value of the $j$-th feature of the $i$-th centroid ($i$ = 1 ... $k$; $j$ = 1 ... $d$).


\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{images/centroids-encoding}
    \caption{The encoding based on centroids features}\label{fig:centroids-encoding}
  \end{center}
\end{figure}

%An encoding based on the centroids description is also considered in order to treat the redundancy that may arise from the object-cluster representation. For instance, the solutions (1,1,3,2) and (3,3,1,2) may be considered different solutions according to the object-cluster assignment, even though they are exactly the same solution. Thus, a centroids encoding is used to represent solutions whenever this redundancy must be detected, for example, when performing the crossover.

\begin{algorithm}[H]
\caption{Genetic algorithm framework}
\label{genetic-algo}
\begin{algorithmic}[1]
\STATE Initialize population
\WHILE{number of iterations without improvement $< itNoImprovement$}
\STATE Select parents $p_1$ and $p_2$
\STATE Generate an offspring $\theta$ from $p_1$ and $p_2$ (crossover)
\STATE Generate an individual $\theta'$ by mutating $\theta$ (mutation)
\STATE Apply local search on $\theta'$
\STATE Add $\theta$ and $\theta'$ to the population
\IF{population size = $maxPop$}
\STATE Select survivors
\IF{best solution not improved for $itDiv$ iterations}
\STATE Diversify population
\ENDIF
\ENDIF
\ENDWHILE
\STATE Return best solution
\end{algorithmic}
\end{algorithm}

\section{Initial population}
The first step in a GA is the generation of an initial population. It has been recognized that if the initial population of the GA is good, then the algorithm has a better possibility of finding a good solution \cite{Burke2004}, \cite{Zitzler2000}. Some factors can influence the initial population or should be taken into account when an initial population is generated randomly: the search space, the fitness function, the diversity, and the number of individuals \cite{DiazGomez2007}.

In this work, the initial population is randomly generated, by assigning each object to a cluster according to a discrete uniform distribution, i.e., where each outcome is equally likely to happen. To compose the initial population, $\mu$ individuals are created. Then, each initial individual is submitted to a local search.

\section{Individuals management}
In order to increase the population and start the competitive process of evolution we need to keep introducing new individuals. The generation of a new individual (offspring) begins with the random selection of two parents, $p_1$ and $p_2$, which are submitted to a crossover procedure that generates a child individual $\theta$. Then, $\theta$ is added to the population. As highly fit solutions have more chances to be selected for reproduction, the offspring -- which combines characteristics from each parent -- is likely to have a good fitness. Thus, we also consider the mutation operator, which generates a new individual $\theta'$ that is similar to $\theta$ and tends to be good regarding the fitness.

\subsection{Selection}
The selection is the stage where individuals from the population are chosen to mate and generate a new individual. Due to the evolutionary behaviour of GA, the most likely individuals to be chosen for selection are the ones with good fitness. That way, the good genes can be propagated to generate good children solutions. In this work, the parent selection is done through a binary tournament, which randomly selects $w$ individuals (with uniform probability) from the population and keeps the one with the best fitness among the $w$ individuals to set the first parent. The fitness here considered is the value of the objective function (cost) of a solution. Then, the same selection scheme is performed to set the second parent.

\subsection{Crossover}
Once parent solutions $p_1$ and $p_2$ were selected, they should be submitted to a crossover procedure in order to produce a offspring (child) solution from them:

\begin{enumerate}
	\item Firstly, the minimum bipartite matching between centroids of $p_1$ and $p_2$ is calculated. One set of nodes in the matching problem is composed by the centroids of $p_1$ and the other set is composed by the centroids of $p_2$. The goal is to produce the one-to-one assignment of centroids from different sets, in such a way that the overall edges weight (distances) is minimized. This matching problem is solved by the Hungarian method and leads to $k$ pairs of centroids, where $k$ is the number of clusters (centroids).

	\item For each pair of centroids resulted from the matching solution, one of them is randomly selected and set as a centroid of the offspring solution.

	\item Finally, data points are assigned to the closest offspring centroid.
\end{enumerate}

\subsection{Mutation}
The mutation of a solution is done by a biased relocation of a centroid. This strategy proved effective as it introduces some drastic movement in one centroid position, allowing the overall solution to have considerable changes. The following items describe how the mutation works:

\begin{enumerate}

	\item Randomly select a centroid $c^{*}$ and remove it from the solution.
	
	\item Among the $m-1$ remaining centroids, re-assign each data point to the closest centroid.
	
	\item Randomly select the position of a data point and re-insert $c^{*}$ there. This position is selected as in a wheel roulette, such that data points far from their centroids are more likely to be chosen.
	
	\item Among the $m$ resulting centroids, re-assign each data point to the closest centroid.
		
\end{enumerate}

\subsection{Local improvement}
Once an offspring $\theta$ was generated by crossover, it is improved through a local search procedure. The local search aims to find a local optimal by applying local changes to the current solution. Here, the adopted local search is one run of the k-means algorithm. The k-means algorithm starts with an initial solution with $k$ centroids $c_1$, $c_2$, ..., $c_k$, and proceeds by alternating between two steps:

\begin{itemize}

	\item Assignment step: Assign each data point to the closest cluster.

	\item Update step: Calculate the new centroids to be the mean (average) point of the data points in the new clusters.
	
\end{itemize}

Then the algorithm keeps repeating these two steps until the assignments no longer change, converging to a local optima.

In our experiments, we used the k-means implementation of Greg Hamerly \cite{Hamerly2010}, who proposed an acceleration that gives the same answer of the standard Lloyd's k-means but is much faster in practice. This implementation avoids distance computations by using the triangle inequality and lower bounds on distances. The time per k-means iteration of the algorithm is O($ndk$+$dk^2$), where $n$ is the number of data points, $d$ is the number of data dimensions (features) and $k$ is the number of clusters. However, the calculated lower bounds allow to eliminate the innermost k-means loop in around 80\% of the time, which in practise is much faster than the standard k-means.

\section{Population management and termination}
Two mechanisms were developed in order to complement the selection, crossover and local improvement operators: survivors selection and diversification management.

\subsection{Survivors selection}
The \textit{Survivors selection} aims to select the best individuals to propagate when the maximum population size is achieved. This procedure determines the $\mu$ individuals that will go on to the next generation, by discarding $\lambda$ individuals ($\lambda = maxPop - \mu$) that are either clones or bad regarding the fitness, according to algorithm \ref{survivors}.

\begin{algorithm}[H]
\caption{Survivors selection}
\label{survivors}
\begin{algorithmic}[1]
\FOR{$i = 1 ... \lambda$}
\STATE $X \leftarrow $ all individuals having a clone
\IF{$X \neq \emptyset$}
\STATE Remove $p \in X$ with maximum fitness
\ELSE
\STATE Remove $p$ in the population with maximum fitness
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Diversification management}
The \textit{Diversification} mechanism aims to ensure the diversity of the population. It is called after the survivors selection whenever \textit{itDiv} iterations happen without improving the best solution. It is performed by creating $\beta$ new individuals as in the initialization phase, i.e., individuals are randomly generated and then submitted to local search.

\section{Parameters}
In preliminary experiments, we determined that the values $\mu$ = 20, $maxPop$ = 500, $itDiv$ = 400 and $itNoImprovement$ = 2000 produced good and stable results, where $\mu$ is the population size, $maxPop$ is the maximum size of population, $itDiv$ is the number of iterations without improvement that triggers diversification and $itNoImprovement$ is the number of iterations without improvement that causes the algorithm stop.

\section{Computational complexity}