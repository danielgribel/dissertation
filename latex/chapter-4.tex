\chapter{Computational Experiments and Analysis}
\label{chap:experiments}
In this chapter, we discuss the computational experiments and the analysis that emerge from them. The results of the proposed algorithm are presented in terms of objective function value and time, and are also compared to current state-of-the-art results in the MSSC literature.

The experiments were conducted on an Intel Core i5 2.6 GHz processor machine with 8 GB of RAM memory. The source codes were written in C++, using the UNIX g++ compiler on a Linux Ubuntu 14.04 LTS 64-bit operating system.

Two different analysis are presented in this chapter. The first one analyses the performance of the proposed algorithm in terms of objective function value and computational time, where we compare our results with the current literature. The second one analyses the impact of instances characteristics, where the objective is to verify if the proposed algorithm is affected by the number of clusters and the dimensionality of data.

\section{Instances}
\label{sec:instances}
The selection of instances was done in order to cover different types of real data, considering both the instance size and the number of features as important indicators to describe the nature of data. Table \ref{instances} summarizes the characteristics of the adopted instances, which correspond to XX important benchmarks in recent clustering literature, as reported in \cite{Ordin2014} and \cite{Bagirov2016}. For all considered instances, every feature value is a real or integer number and there is no missing values. The number of data points (instances size) ranges from 59 (smallest) to 434,874 (largest); and the number of features ranges from 2 (smallest) to 128 (largest). Thus, in order to elucidate the coming analysis, we propose the discrimination of these instances according to their sizes -- having small, medium and large sets. % and the number of features (small and large sets)

\input{tables/instances}

\section{Parameters calibration}
\label{sec:calibration}
As in most meta-heuristics, the values chosen for parameters directly affect the results. In this work, we consider six main parameters: $w$ (the tournament size for selection), $\mu$ (the population size), $\Pi$ (the maximum size of population), $\beta$ (the number of new individuals created in the diversification), $I_D$ (the number of iterations without improvement that triggers diversification) and $I_S$ (the number of iterations without improvement that causes the algorithm stop). For the parameters related to the management of individuals in the population ($w$, $\mu$, $\Pi$ and $\beta$) we performed a calibration to choose the best configuration for the experiments. For the remaining parameters $I_D$ and $I_S$, we directly set their values to 400 and 3000, respectively.

In preliminary experiments, we started with the following configuration for the free variables, as they produced good and stable results in an initial and manually exploration: $w$ = 3, $\mu$ = 30, $\Pi$ = 200 and $\beta$ = 10. From this baseline, we expanded the range of these values. Table \ref{calibration} presents the tested values for each parameter and the final values achieved after running each combination in a subset of 5 instances.

The 5 instances for calibration were chosen based on their sizes and number of features. Small instances were not considered because in almost all of them the different configurations for calibration achieved the best known result, so they are not so informative. As we consider 3 parameters and a range of 3 possible values for each, the combination of parameters resulted in 27 possible configurations ($3^3$). For this reason, large instances were not considered also. It would take too much time to calibrate, as we run 10 times each experiment. So we chose medium-sized instances and tested 4 different number of clusters (20, 30, 40, 50) for each instance. After measuring the offset between the average error and the execution time for each configuration, we took the best one among the 27 options.

\input{tables/calibration}

\section{Results}
\label{sec:results}
Results of numerical experiments for small, medium and large instances are given in tables \ref{results-all-A}, \ref{results-all-B} and \ref{results-all-C} respectively. Due to the stochastic nature of the proposed meta-heuristic, the results of GA-KM correspond to the average of 10 runs, where a run is the execution of an instance with a specific number of clusters $m$. The following notation is used in these tables:

\begin{itemize}

	\item $m$ is the number of clusters;

	\item $f_{best}$ is the best known value for the MSSC objective found so far;

	\item $E$ is the error from the best known solution, calculated as:

		\begin{center}
		\large
			$E = \frac{f - f_{best}}{f_{best}} \times 100$
		\end{center}
		
	where $f$ is the value of the MSSC objective found by an algorithm;
	
	\item $t$ is the CPU time in seconds;

	%\item Bold values corresponds to cases where the proposed meta-heuristic found the new best solution.

\end{itemize}

For comparison purposes, we analyse the results considering the best known solutions found so far and the results of global k-means (GKM), the modified global k-means (MGKM), the multi-start modified global k-means (MS-MGKM) and the difference of convex clustering (DCC) algorithms, that are recent works in MSSC literature, corresponding in many cases to the state-of-the-art for this problem.

\input{tables/results-all-A}

\input{tables/results-all-B}

\input{tables/results-all-C}

Table \ref{results-all-A} presents the results for instances with less than 1,000 data points. As we can see, GA-KM found the new best solution or achieved the current best one in all instances for most of $m$ values. In some cases, as in \textit{Ionosphere} instance, GA-KM results reach more than 4\% of the error, being a significant improvement. On the other hand, the proposed algorithm requires significantly more computational time than MS-MGKM, MGKM and GKM algorithms for these small instances. It is also important to note that in some instances like \textit{Bavaria postal 2}, \textit{Liver disorders}, \textit{Ionosphere} and \textit{Pima Indians diabetes} GA-KM results increase the gap from the best known solutions for large $m$, which means that for harder clustering tasks GA-KM produces significantly better solutions than the other algorithms.

Table \ref{results-all-B} presents the results for instances where the number of data points ranges from  1,060 to 20,000. As in results for small instances, GA-KM found the new best solution or achieved the current best one in all instances for most of $m$ values.

Table \ref{results-all-C} presents the results for instances where the number of data points ranges from  13,910 to 434,874. As in the previous results, GA-KM found the new best solution or achieved the current best one in all instances for most of $m$ values.

%The GA-KM algorithm is slower than all compared algorithms when $m$ is large. However, when $m$ is small, GA-KM is faster than MS-MGKM and GKM -- for example, in instance TSPLIB3038 where $m = 2$.

\begin{figure}[h!]
\centering
\subfigure[\textit{TSPLIB3038} instance ($d = 2$).]{\label{fig:a}\includegraphics[width=0.45\textwidth]{img/time-tsplib3038}}
\subfigure[\textit{Liver} instance ($d = 6$).]{\label{fig:a}\includegraphics[width=0.45\textwidth]{img/time-liver}}
\subfigure[\textit{Image segmentation} instance ($d = 19$).]{\label{fig:b}\includegraphics[width=0.45\textwidth]{img/time-image}}
\subfigure[\textit{Ionosphere} instance ($d = 34$).]{\label{fig:b}\includegraphics[width=0.45\textwidth]{img/time-ionosphere}}
\caption{The CPU time of algorithms vs the number of clusters when considering instances with different number of features ($d$)}
\end{figure}

\noindent [TO-DO] results table for group C of instances (all components) -- need to get the exact times for the DCClust and MS-DCA algorithms

\noindent [TO-DO] analysis of the results -- our algorithm beats the benchmarks in many cases

\noindent [TO-DO] analysis on number of features -- our algorithm seems to be fast when $d$ is small in small/medium instances (tsplib for example) but also when $d$ is large in large instances (gas sensor for example)

\noindent [TO-DO] analysis on number of clusters -- how the algorithm is affected if we increase the number of clusters

\noindent [TO-DO] results table for group A of instances (- mutation)

\noindent [TO-DO] results table for group B of instances (- mutation)

\noindent [TO-DO] results table for group C of instances (- mutation)

\noindent [TO-DO] results table for group A of instances (- diversification)

\noindent [TO-DO] results table for group B of instances (- diversification)

\noindent [TO-DO] results table for group C of instances (- diversification)