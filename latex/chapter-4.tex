\chapter{Computational Experiments and Analysis}
\label{chap:experiments}
In this chapter, we discuss the computational experiments and the analysis that emerge from them. From now on, all results regarding the proposed algorithm will be identified as HGKM (for Hybrid Genetic K-Means). The results are presented in terms of objective function value and time, and are compared to current state-of-the-art results in the MSSC literature.

The experiments were conducted on an Intel Core i5 2.6 GHz processor machine with 8 GB of RAM memory. The source codes were written in C++, using the UNIX g++ compiler on a Linux Ubuntu 14.04 LTS 64-bit operating system.

Two different analysis are presented in this chapter. The first one analyses the performance of the proposed algorithm in terms of objective function value and computational time, where we compare our results with the current literature. The second one analyses the impact of instances characteristics, where the objective is to verify if the proposed algorithm is affected by the number of clusters and the dimensionality of data.

\section{Instances}
\label{sec:instances}
The selection of instances was done in order to cover different types of real data, considering both the instance size and the number of features as important indicators to describe the nature of data. Table \ref{instances} summarizes the characteristics of the adopted instances, which correspond to 24 important benchmarks in recent clustering literature, as reported in \cite{Ordin2014} and \cite{Bagirov2016}. For all considered instances, every feature value is a real or integer number and there is no missing values. The number of data points (instances size) ranges from 59 (smallest) to 434,874 (largest); and the number of features ranges from 2 (smallest) to 128 (largest).

\input{tables/instances}

In order to elucidate the coming analysis, we propose the discrimination of these instances according to their sizes, that from now on will by identified as Group A1, Group A2, Group B and Group C of instances.

Groups A1 and A2 correspond to small instances reported in the work of \cite{Ordin2014}. The difference between them is that A1 is composed by really small instances with up to 150 data points, where instances in A2 have some hundreds of data points. This distinction is useful to choose a reasonable number of clusters to be tested in each group. For instances A1, tests are performed considering up to 10 clusters, whereas in A2 we can extend a bit the number of clusters. Group B of instances correspond to medium/large instances reported also by \cite{Ordin2014}, comprising data where the number of points ranges from 1,060 to 20,000. Group C of instances are the same reported in \cite{Bagirov2016}, where instance sizes ranges from 13,910 to 434,874 data points. This last group is especially relevant as it consider large real world instances, which allows a deeper analysis on algorithms scaling.

\section{Parameters calibration}
\label{sec:calibration}
As in most meta-heuristics, the values chosen for parameters directly affect the results. In this work, we consider five main parameters: $w$ (the tournament size for selection), $\mu$ (the population size), $\Pi$ (the maximum size of population), $I_{max}$ (the maximum number of iterations) and $I_S$ (the number of iterations without improvement that causes the algorithm stop). For the parameters related to the management of individuals in the population ($w$, $\mu$ and $\Pi$) we performed a calibration to choose the best configuration for the experiments. For the parameters related to the number of iterations the algorithm takes ($I_{max}$ and $I_S$), we directly set their values to 4000 and 2500, respectively.

In preliminary experiments, we started with the following configuration for the free variables, as they produced good and stable results after an initial and manually exploration: $w$ = 3, $\mu$ = 80 and $\Pi$ = 300. From this baseline, we expanded the range of these values. Table \ref{calibration} presents the ranges of tested values for each parameter and the final values achieved after increasing/decreasing each parameter value at a time.

\input{tables/calibration}

We considered a subset of five instances for calibration, that were chosen based on their sizes and number of features (Liver disorders, Ionosphere, Breast cancer, Pima Indians diabetes and TSPLIB1060). Small instances were not considered because in almost all of them the different configurations for calibration achieved the best known result (possibly the global optimal), so they are not so informative. As we consider 3 parameters and their ranges of values, the combination of parameters resulted in 11 possible configurations, as shown in table \ref{calibration-results}. For this reason, large instances were not considered also. It would take too much time just to calibrate the algorithm, as we run 10 times each experiment. So we chose medium-sized instances and tested 4 different number of clusters (20, 30, 40, 50) for each instance. After measuring the offset between the average error and the execution time for each configuration, we took the best one among the 11 options.

In table \ref{calibration-results}, the first three columns report the values of parameters $w$, $\mu$ and $\Pi$, whereas the remaining columns report, in turn, the errors from the best, worst, median and average solution, and finally the total time taken to run all the considered instances for calibration.

\input{tables/calibration-results}

\section{Experimental results}
\label{sec:results}

\subsection{Performance and time}
\label{sec:performance}
Results of numerical experiments regarding solution performance and computational time for groups A1, A2, B and C of instances are presented in tables \ref{results-all-A1}, \ref{results-all-A2}, \ref{results-all-B} and \ref{results-all-C}, respectively. Due to the stochastic nature of the proposed meta-heuristic, the results of HGKM correspond to the average of 10 runs, where a run is the execution of an instance with a specific number of clusters $m$. The following notation is used in these tables:

\begin{itemize}

	\item $m$ is the number of clusters;

	\item $f_{best}$ is the best known value for the MSSC objective found so far;

	\item $E$ is the error from the best known solution, calculated as:

		\begin{center}
		\large
			$E = \frac{f - f_{best}}{f_{best}} \times 100$
		\end{center}
		
	where $f$ is the value of the MSSC objective found by an algorithm. For HGKM, $E_{med}$ and $E_{avg}$ report the error from the median and average values, respectively;
	
	\item $t$ is the CPU time in seconds;

	%\item Bold values corresponds to cases where the proposed meta-heuristic found the new best solution.

\end{itemize}

For comparison purposes, we analyse the results considering the best known solutions found so far and the results of global k-means (GKM), the modified global k-means (MGKM), the multi-start modified global k-means (MS-MGKM) and the difference of convex clustering (DCC) algorithms, which are recent works in MSSC literature, corresponding in many cases to the state-of-the-art for this problem.

% HGKM performance analysis
Tables \ref{results-all-A1} and \ref{results-all-A2} presents the results for instances of group A1 and A2. As we can observe, HGKM finds the new best solution or achieves the current best one in all instances for most of $m$ values. In some cases, as in Ionosphere instance, HGKM results reach more than 4\% of the error, being a significant improvement. On the other hand, the proposed algorithm requires more computational efforts than MS-MGKM, MGKM and GKM algorithms for most of these small instances -- in many cases the computational time of HGKM is similar to MS-MGKM. It is also important to note that in some instances like Bavaria postal 2, Liver disorders, Ionosphere and Pima Indians diabetes, HGKM results increase the gap from the best known solutions for large $m$, which means that for harder clustering tasks HGKM produces significantly better solutions than the compared algorithms.

Table \ref{results-all-B} presents the results for instances of group B. As in results for small instances, HGKM finds the new best solution or achieves the current best one in all instances for most of $m$ values, being better than algorithms MS-MGKM, MGKM and GKM in terms of solution quality. Regarding computational time, HGKM is faster than MS-MGKM and GKM in TSPLIB3038, faster than MS-MGKM in TSPLIB1060 and has nearly the same time of MS-MGKM and MGKM in Pendigit. For Image segmentation and Page Block -- the later, only for large $m$ -- HGKM is the algorithm that requires more computational effort.

Table \ref{results-all-C} presents the results for instances of group C -- the group with the largest instances. As in the previous results, HGKM finds the new best solution or achieves the current best one in all instances for most of $m$ values. Regarding computational time, HGKM is faster than MS-MGKM, DCCClust, MS-DCA and GKM for almost all instances, except for EEG eye state and D15112.

\input{tables/results-all-A1}

\input{tables/results-all-A2}

\input{tables/results-all-B}

\input{tables/results-all-C}

\subsection{Number of clusters}
From the performance of HGKM in both solution quality and computational time, we sought to verify if the algorithm works well by increasing the number of clusters. This analysis is very important, since clustering tasks that require more groups are, in general, more difficult tasks, since we are dealing with a larger combinatorial problem.

Table \ref{m-analysis} presents a summary on how HGKM perform for different numbers of clusters.
For each group of instances, it reports the average error ($E_{avg}$) to the best known solution when varying the number of clusters ($m$). For groups of instances A1, A2 and B, HGKM has the most significant improvements -- compared to the best known solution -- in cases where $m$ is large, which indicates that the proposed method works well in harder clustering tasks. For instances of group C, HGKM increased the gap to the best known solution when $m$ = 2, 20, 25, confirming the robustness of the method.

\input{tables/m-analysis}

\subsection{Data dimensionality}
From results of section \ref{sec:performance}, we can observe that HGKM outperforms the other algorithms in both low and high dimensions instances. In order to demonstrate how HGKM behaves in terms of computational time if we consider instances with high dimensionality, figure \ref{fig:dimen} shows a comparison to MS-MGKM, MGKM and GKM in some instances with different number of features.

\noindent [TO-DO] complete the analysis: up to now is not conclusive -- maybe complete with large instances not executed yet

\begin{figure}[H]
\centering
\subfigure[\textit{TSPLIB3038} ($d = 2$)]{\label{fig:dimen-a}\includegraphics[width=0.45\textwidth]{img/time-tsplib3038}}
\subfigure[\textit{Liver} ($d = 6$)]{\label{fig:dimen-b}\includegraphics[width=0.45\textwidth]{img/time-liver}}
\subfigure[\textit{Heart} ($d = 13$)]{\label{fig:dimen-c}\includegraphics[width=0.45\textwidth]{img/time-heart}}
\subfigure[\textit{Pendigit} ($d = 16$)]{\label{fig:dimen-d}\includegraphics[width=0.45\textwidth]{img/time-pendigit}}
\subfigure[\textit{Image segmentation} ($d = 19$)]{\label{fig:dimen-e}\includegraphics[width=0.45\textwidth]{img/time-image}}
\subfigure[\textit{Ionosphere} ($d = 34$)]{\label{fig:dimen-f}\includegraphics[width=0.45\textwidth]{img/time-ionosphere}}
\caption{The CPU time of algorithms vs the number of clusters when considering instances with different number of features ($d$)}
\label{fig:dimen}
\end{figure}

\noindent [TO-DO] the exact times for the DCClust and MS-DCA algorithms in group C of instances

%\noindent [TO-DO] analysis on number of features -- our algorithm seems to be fast when $d$ is small in small/medium instances (tsplib for example) but also when $d$ is large in large instances (gas sensor for example)

\noindent [TO-DO] results without mutation

\noindent [TO-DO] results without crossover