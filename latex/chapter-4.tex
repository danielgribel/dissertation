\chapter{Computational Experiments and Analysis}
\label{chap:experiments}
In this chapter, we discuss the computational experiments and the analysis that emerge from them. From now on, all results regarding the proposed algorithm will be identified as HGKM (for Hybrid Genetic K-Means). The results are presented in terms of objective function value and time, and are compared to current state-of-the-art results in the MSSC literature.

The experiments were conducted on an Intel Core i5 2.6 GHz processor machine with 8 GB of RAM memory. The source codes were written in C++, using the UNIX g++ compiler on a Linux Ubuntu 14.04 LTS 64-bit operating system.

Three different analysis are presented in this chapter. The first one analyses the general performance of the proposed algorithm in terms of objective function value and computational time, where we compare our results with the current literature. The second one analyses the impact of instances characteristics, where the objective is to verify if the proposed algorithm is affected by the number of clusters and the size of instances. Finally, we analyse the contribution of crossover and mutation components in the performance of the method.

In the coming result tables, the following notation is used:

\begin{itemize}
	\item $n$ is the size of the instance (number of data points);
	
	\item $m$ is the number of clusters;

	\item $d$ is the number of features;
	
	\item $f_{best}$ is the best known value for the MSSC objective found so far;

	\item $E$ is the error from the best known solution, calculated as:

		\begin{center}
		\large
			$E = \frac{f - f_{best}}{f_{best}} \times 100$
		\end{center}
		
	where $f$ is the value of the MSSC objective found by an algorithm. For HGKM, $E_{med}$ and $E_{avg}$ report the error from the median and average values, respectively;
	
	\item $t$ is the CPU time in seconds;
	
	%\item Bold values are used when HGKM found the new best solution

\end{itemize}

\section{Instances}
\label{sec:instances}
The selection of instances was done in order to cover different types of real data, considering both the instance size and the number of features as important indicators to describe the nature of data. Table \ref{instances} summarizes the characteristics of the adopted instances, which correspond to 24 important benchmarks in recent clustering literature, as reported in \cite{Ordin2014} and \cite{Bagirov2016}. For all considered instances, every feature value is a real or integer number and there is no missing values. The number of data points (instances size) ranges from 59 (smallest) to 434,874 (largest); and the number of features ranges from 2 (smallest) to 128 (largest).

In order to elucidate the coming analysis, we propose the discrimination of these instances according to their sizes, that from now on will by identified as Group A1, Group A2, Group B and Group C of instances.

Groups A1 and A2 correspond to small instances reported in the work of \cite{Ordin2014}. The difference between them is that A1 is composed by really small instances with up to 150 data points, where instances in A2 have some hundreds of data points. This distinction is useful to choose a reasonable number of clusters to be tested in each group. For instances A1, tests are performed considering up to 10 clusters, whereas in A2 we can extend a bit the number of clusters. Group B of instances correspond to medium/large instances reported also by \cite{Ordin2014}, comprising data where the number of points ranges from 1,060 to 20,000. Group C of instances are the same reported in \cite{Bagirov2016}, where instance sizes ranges from 13,910 to 434,874 data points. This last group is especially relevant as it consider large real world instances, which allows a deeper analysis in the scaling of algorithms.

\input{tables/instances}

\section{Parameters calibration}
\label{sec:calibration}
As in most meta-heuristics, the values chosen for parameters directly affect the results. In this work, we consider five main parameters: $w$ (the tournament size for selection), $\mu$ (the population size), $\Pi$ (the maximum size of population), $I_{max}$ (the maximum number of iterations) and $I_S$ (the number of iterations without improvement that causes the algorithm stop). For the parameters related to the management of individuals in the population ($w$, $\mu$ and $\Pi$) we performed a calibration to choose the best configuration for the experiments. For the parameters related to the number of iterations the algorithm takes ($I_{max}$ and $I_S$), we directly set their values to 4000 and 2500, respectively. These values were set in order to obtain results in a time comparable to previous authors.

In preliminary experiments, we started with the following configuration for the free variables, as they produced good and stable results after an initial and manually exploration: $w$ = 3, $\mu$ = 80 and $\Pi$ = 300. From this baseline, we expanded the range of these values by an one-factor-at-a-time (OFAT) approach. Table \ref{calibration} presents the ranges of tested values for each parameter and the final values achieved after increasing/decreasing each parameter value at a time.

\input{tables/calibration}

We considered a subset of five instances for calibration, that were chosen based on their sizes and number of features (Liver disorders, Ionosphere, Breast cancer, Pima Indians diabetes and TSPLIB1060). Small instances were not considered because in almost all of them the different configurations for calibration achieved the best known result (possibly the global optimal), so they are not so informative. As we consider 3 parameters independently, the combination of parameters resulted in 11 possible configurations, as shown in table \ref{calibration-results}. For this reason, large instances were not considered.
%It would take too much time just to calibrate the algorithm, as we run 10 times each experiment.
We chose medium-sized instances and tested 4 different number of clusters (20, 30, 40, 50) for each instance. After measuring the offset between the average error and the execution time for each configuration, we took the best one among the 11 options.

In table \ref{calibration-results}, the first three columns report the values of parameters $w$, $\mu$ and $\Pi$, whereas the remaining columns report, in turn, the errors from the best, worst, median and average solution, and finally the average time taken to run the considered instances for calibration.

\input{tables/calibration-results}

\section{Experimental results}
\label{sec:results}

\subsection{General performance and computational time}
\label{sec:performance}
The results of numerical experiments regarding solution performance and computational time for groups A1, A2, B and C of instances are presented in tables \ref{results-all-A1}, \ref{results-all-A2}, \ref{results-all-B} and \ref{results-all-C}, respectively. Due to the stochastic nature of the proposed meta-heuristic, the results of HGKM correspond to the average of 10 runs, where a run is the execution of an instance with a specific $m$. 

For comparison purposes, we analyse the results considering the best known solutions found so far and the results of global K-means (GKM) \cite{Likas2003}, the modified global K-means (MGKM) \cite{Bagirov2008}, the multi-start modified global K-means (MS-MGKM) \cite{Ordin2014} and the difference of convex clustering (DCC) \cite{Bagirov2016} algorithms, which are recent works in MSSC literature, corresponding in many cases to the state-of-the-art for this problem.

% HGKM performance analysis
Tables \ref{results-all-A1} and \ref{results-all-A2} presents the results for instances of group A1 and A2. As we can observe, HGKM finds the new best solution or achieves the current best one in all instances for most of $m$ values. In some cases, as in Ionosphere instance, HGKM results reach more than 4\% of the error, being a significant improvement. On the other hand, the proposed algorithm requires more computational efforts than MS-MGKM, MGKM and GKM algorithms for most of these small instances -- in many cases the computational time of HGKM is similar to MS-MGKM. It is also important to note that in some instances like Bavaria postal 2, Liver disorders, Ionosphere and Pima Indians diabetes, the gap of HGKM results to the best known solutions increases for large $m$, which means that for harder clustering tasks HGKM produces significantly better solutions than the compared algorithms.

Table \ref{results-all-B} presents the results for instances of group B. As in results for small instances, HGKM finds the new best solution or achieves the current best one in all instances for most of $m$ values, being better than algorithms MS-MGKM, MGKM and GKM in terms of solution quality. Regarding computational time, HGKM is faster than MS-MGKM and GKM in TSPLIB3038, faster than MS-MGKM in TSPLIB1060 and has nearly the same time of MS-MGKM and MGKM in Pendigit. For Image segmentation and Page Block -- the latter, only for large $m$ -- HGKM requires more computational effort.

Table \ref{results-all-C} presents the results for instances of group C -- the group with the largest instances. As in the previous results, HGKM finds the new best solution or achieves the current best one in all instances for most of $m$ values. Regarding computational time, HGKM is faster than MS-MGKM, DCCClust, MS-DCA and GKM for almost all instances, except for EEG eye state and D15112.

\input{tables/results-all-A1}

\input{tables/results-all-A2}

\input{tables/results-all-B}

\input{tables/results-all-C}

\subsection{Number of clusters}

From the performance observed in the results of HGKM algorithm in both solution quality and computational time, we sought to verify if the algorithm works well by increasing the number of clusters. %This analysis is very important, since clustering tasks that require more groups are, in general, more difficult tasks, since we are dealing with a larger combinatorial problem.
This analysis is very important to confirm the robustness of the method, since we are dealing with a larger combinatorial problem when $m$ is large.

Table \ref{m-analysis} presents a summary on how HGKM perform for different numbers of clusters.
For each group of instances, it reports the average error ($E_{avg}$) to the best known solution when varying the number of clusters ($m$). For groups of instances A1, A2 and B, HGKM has the most significant improvements -- compared to the best known solution -- in cases where $m$ is large. For instances of group C, HGKM increased the gap to the best known solution when $m$ = 2, 20, 25, confirming the robustness of the method.

\input{tables/m-analysis}

%\subsection{Data dimensionality}
%From results of section \ref{sec:performance}, we can observe that HGKM outperforms the other algorithms in both low and high dimensions instances. In order to demonstrate how HGKM behaves in terms of computational time if we consider instances with high dimensionality, figure \ref{fig:dimen} shows a comparison to MS-MGKM, MGKM and GKM in some instances with different number of features.

%\begin{figure}[H]
%\centering
%\subfigure[\textit{TSPLIB3038} ($d = 2$)]{\label{fig:dimen-a}\includegraphics[width=0.45\textwidth]{img/time-tsplib3038}}
%\subfigure[\textit{Liver} ($d = 6$)]{\label{fig:dimen-b}\includegraphics[width=0.45\textwidth]{img/time-liver}}
%\subfigure[\textit{Page blocks} ($d = 10$)]{\label{fig:dimen-c}\includegraphics[width=0.45\textwidth]{img/time-page}}
%\subfigure[\textit{Heart} ($d = 13$)]{\label{fig:dimen-d}\includegraphics[width=0.45\textwidth]{img/time-heart}}
%\subfigure[\textit{Pendigit} ($d = 16$)]{\label{fig:dimen-e}\includegraphics[width=0.45\textwidth]{img/time-pendigit}}
%\subfigure[\textit{Letters} ($d = 16$)]{\label{fig:dimen-f}\includegraphics[width=0.45\textwidth]{img/time-letters}}
%\subfigure[\textit{Image segmentation} ($d = 19$)]{\label{fig:dimen-g}\includegraphics[width=0.45\textwidth]{img/time-image}}
%\subfigure[\textit{Ionosphere} ($d = 34$)]{\label{fig:dimen-h}\includegraphics[width=0.45\textwidth]{img/time-ionosphere}}
%\caption{The CPU time of algorithms vs the number of clusters when considering instances with different number of features ($d$)}
%\label{fig:dimen}
%\end{figure}

\subsection{Instances size}
In section \ref{sec:performance} we shown that the objective value of solutions produced by HGKM outperformed the compared algorithms in most experiments. In this section, we present some graphs regarding HGKM scaling, i.e., how much computational time the algorithm requires in large data. Figures \ref{fig:sizeA2}, \ref{fig:sizeB} and \ref{fig:sizeC} compare HGKM with MS-MGKM, MGKM, GKM and DCClust for groups A2, B and C of instances and $m$ = (2, 10, 20). The computational time was measured in seconds and is presented in the logarithmic scale. Here we do not present the results for group A1 as the difference in computational time is really small. For each group, the instances are positioned in ascending order of size. That is, the first instance (positioned to the left) is the smallest one, while the last is the largest one.

For instances of group A2, HGKM demands more computational efforts than MS-MGKM, MGKM, GKM and DCClust for all values of $m$. For group B of instances, HGKM takes a similar amount of time when compared to other algorithms, and has its most competitive performance when $m$ is increased. For instances of group C, HGKM is faster than DCClust exactly in the largest instances (Pla85900, Skin segmentation and 3D road), having a similar computational time for the remaining instances in this group.

Therefore, HGKM is very promising regarding scaling, as it produces better solutions and requires less computational time than the compared algorithms in large instances. In addition, there is room to use more efficient data structures that can improve the execution time of the algorithm.

\begin{figure}[H]
\centering
\subfigure[$m$ = 2]{\label{fig:sizeA2-2}\includegraphics[width=1\textwidth]{img/sizeA2-2}}
\subfigure[$m$ = 10]{\label{fig:sizeA2-10}\includegraphics[width=1\textwidth]{img/sizeA2-10}}
\subfigure[$m$ = 20]{\label{fig:sizeA2-20}\includegraphics[width=1\textwidth]{img/sizeA2-20}}
\caption{The CPU time of algorithms in Instances A2}
\label{fig:sizeA2}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[$m$ = 2]{\label{fig:sizeB-2}\includegraphics[width=1\textwidth]{img/sizeB-2}}
\subfigure[$m$ = 10]{\label{fig:sizeB-10}\includegraphics[width=1\textwidth]{img/sizeB-10}}
\subfigure[$m$ = 20]{\label{fig:sizeB-20}\includegraphics[width=1\textwidth]{img/sizeB-20}}
\caption{The CPU time of algorithms in Instances B}
\label{fig:sizeB}
\end{figure}

\begin{figure}[H]
\centering
\subfigure[$m$ = 2]{\label{fig:sizeC-2}\includegraphics[width=1\textwidth]{img/sizeC-2}}
\subfigure[$m$ = 10]{\label{fig:sizeC-10}\includegraphics[width=1\textwidth]{img/sizeC-10}}
\subfigure[$m$ = 20]{\label{fig:sizeC-20}\includegraphics[width=1\textwidth]{img/sizeC-20}}
\caption{The CPU time of algorithms in Instances C}
\label{fig:sizeC}
\end{figure}

\subsection{Components}
\label{sec:components}
In order to understand the contribution of components in the performance of HGKM, we performed some experiments by removing two important operators at a time: crossover and mutation. For comparison purposes, table \ref{components-table} reports the average error with respect to the best known solution in three scenarios: i) complete HGKM with all components ($E_{avg}$), ii) HGKM without mutation ($E_{avg}$ -M) and iii) HGKM without crossover ($E_{avg}$ -C).

\input{tables/components}

The role of mutation and crossover is very important on finding good solutions. Particularly, for the case where we have HGKM without mutation, it does not find (in average) a solution better than the best known so far, while in the two other scenarios (complete HGKM and HGKM without crossover), it has a significantly better performance. Additionally, the complete HGKM algorithm performs better than the HGKM without crossover, especially when $m$ increases, indicating that the crossover is an essential operator when dealing with more complex clustering tasks.

\noindent [TO-DO] the exact times for the DCClust and MS-DCA algorithms in group C of instances

%\noindent [TO-DO] analysis on number of features -- our algorithm seems to be fast when $d$ is small in small/medium instances (tsplib for example) but also when $d$ is large in large instances (gas sensor for example)