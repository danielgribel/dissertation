\chapter{Computational Experiments and Analysis}
In this chapter, we discuss the computational experiments and the analysis that emerge from them. The results of the proposed algorithm are presented in terms of objective function value and time, and are also compared to current state-of-the-art results in the MSSC literature.

The experiments were conducted on an Intel Core i5 2.6 GHz processor machine with 8 GB of RAM memory. The source codes were written in C++, using the UNIX g++ compiler on a Linux Ubuntu 14.04 LTS 64-bit operating system.

\section{Instances}
The selection of instances was done in order to cover different types of data, considering both the instance size and the number of features as important indicators to describe the nature of data. Table \ref{instances} summarizes the characteristics of the adopted instances, which correspond to XX important benchmarks in recent clustering literature, as reported in \cite{Ordin2014} and \cite{Bagirov2016}. For all considered instances, every feature value is a real or integer number and there is no missing values. The number of data points (instances size) ranges from 59 (smallest) to 434,874 (largest); and the number of features ranges from 2 (smallest) to 128 (largest). Thus, in order to elucidate the coming analysis, we propose the discrimination of these instances according to their sizes -- having small, medium and large sets. % and the number of features (small and large sets)

\input{tables/instances}

\section{Parameters calibration}
As in most meta-heuristics, the values chosen for parameters directly affect the results. In this work, we consider five main parameters: $\mu$ (the population size), $\Pi$ (the maximum size of population), $\beta$ (the number of new individuals created in the diversification), $I_D$ (the number of iterations without improvement that triggers diversification) and $I_S$ (the number of iterations without improvement that causes the algorithm stop). For the parameters related to the management of individuals in the population ($\mu$, $\Pi$ and $\beta$) we performed a calibration to choose the best configuration for the experiments. For the remaining parameters $I_D$ and $I_S$, we directly set their values to 400 and 3000, respectively.

In preliminary experiments, we started with the following configuration for the free variables, as they produced good and stable results: $\mu$ = 30, $\Pi$ = 200 and $\beta$ = 10. From this baseline, we expanded the range of these value. Table \ref{calibration} presents the tested values for each parameter and the final values achieved after running each combination in a subset of 5 instances.

The 5 instances for calibration were chosen based on their sizes and number of features. Small instances were not considered because in almost all of them the different configurations for calibration achieved the best known result, so they are not so informative. As we consider 3 parameters and a range of 3 possible values for each, the combination of parameters resulted in 27 possible configurations ($3^3$). For this reason, large instances were not considered also. It would take too much time to calibrate, as we run 10 times each instance. So we chose medium-sized instances and tested 4 different number of clusters (20, 30, 40, 50) for each instance. After measuring the offset between the average error and the execution time for each configuration, we took the best one among the 27 options.

\input{tables/calibration}

\section{Results}

-- Objective function + time for all instances (all components)

-- Objective function + time (- mutation)

-- Objective function + time (- diversification)

-- Analysis time vs number of clusters (compare to recent literature)

-- Analysis time vs number of features (compare to recent literature): more difficult because we need to compare different instances with almost the same number of points, which could not be so informative due to different nature of data

Results of numerical experiments for small, medium and large instances are given in tables XX, XX and XX respectively. Due to the stochastic nature of the proposed meta-heuristic, 10 runs were conducted for each configuration. The following notation is used in these tables:

\begin{itemize}

	\item $m$ is the number of clusters;

	\item $f_{best}$ is the best known value for the MSSC objective found so far;

	\item $E$ is the error, calculated as:

		\begin{center}
		\Large
			$E = \frac{f - f_{best}}{f_{best}}$
		\end{center}
		
	where $f$ is the value of the MSSC objective found by an algorithm;
	
	\item $t$ is the CPU time in seconds;

	\item Bold values corresponds to cases where the proposed meta-heuristic found the new best solution.

\end{itemize}

For comparison purposes, we analyse our results considering the best known solutions found so far and the results of global k-means (GKM), the modified global k-means (MGKM), the multi-start modified global k-means (MS-MGKM) and the difference of convex clustering (DCC) algorithms, that are recent works in MSSC literature, corresponding in many cases to the state-of-the-art in this problem.