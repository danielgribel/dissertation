% -*- coding: utf-8; -*-
\chapter{Introduction}
Clustering plays an important role in data mining, being useful in many fields that require the separation of objects into meaningful groups. Clustering arises whenever one has a collection of objects or patterns -- say, a set of photographs, documents, or micro-organisms -- and is trying to classify or organize them into coherent groups \cite{tardos}. It has found many applications in fields that deal with exploratory data analysis, such as information retrieval, document extraction, and image segmentation.
%In most situations, the use of clustering algorithms allows the detection of similar objects that could not be easily grouped by humans.

Due to permanent social and physical interactions, human beings have developed the ability of clustering objects, once we are required to do it and draw conclusions from it in a large variety of daily situations. Although the human brain works very well (and quickly) on clustering some objects in two or three dimensions, it is not capable of performing the same task in a very large number of dimensions or in the presence of a large amount of data. Thus, one of the fundamental questions on clustering is: ``How can we propose some means to lead the human cognitive capacity of quickly separating objects in a low dimension of features to a high dimension?''. Most of research on clustering analysis is spending efforts on answering this question in a satisfactory way \cite{Das2009}.

Numerous definitions of clustering analysis have been proposed in the literature. Clustering is typically directed to study the internal structure of data \cite{Karkkainen2006}, but it can assume different definitions and roles according to the domain of study and goals. Even finding a good criteria to validate the output of a clustering algorithm is not trivial. As clustering is a generalization of what humans perceive, it is depicted by many informal definitions and guided by subjective criteria, revealing the difficulty of providing a singular and sufficiently broad definition. Beyond this, clustering criteria are sometimes known and well defined, but results should be given according to a specific and adequate perspective that better attends someone view of the problem. For example, some criteria to validate clusters consider the cohesion and/or separability of patterns. The former asserts that patterns in one cluster should be as similar to each other as possible. The latter asserts that clusters should be well separated, i.e., patterns in different clusters should be as different to each other as possible. These different perspectives have led to the development of different classes of algorithms.

Although they are essential in data mining applications, most clustering algorithms are ad-hoc methods, which can be assimilated to a greedy construction procedure or local search for some optimization problem. These methods are designed for specific clustering criteria, being not adapted to different purposes. In addition, they have no optimality guarantee and usually have volatile performance in the domain of the optimization problem to which they are attached. Thus, these methods work relatively well for certain applications but fail in others.

K-means is an example of a very popular algorithm for clustering, based on iterative updates of cluster centers until convergence. Although these iterative algorithms are easy to implement and usually demand low computational time (linear in the number of samples in k-means), they have typically two disadvantages \cite{Das2009}: (i) they are likely to converge to a local optimum; (ii) their performance is usually very sensitive to the initial conditions of the search; and (iii) they are highly dependent of the shape of clusters.

% Motivation
The main motivation of this work is to overcome the lack of guarantee on the solution quality of classical clustering algorithms. For this purpose, this research addresses the problem of data clustering from an optimization perspective, where we explore a particular formulation, the Minimum sum-of-squares clustering (MSSC) in the Euclidean space. In addition, the limitation imposed by classical ad-hoc methods is in many cases related to premature convergence. To overcome these issues, this work proposes an adaptive meta-heuristic capable of escaping from local minimum and generating near-optimal solutions to the MSSC problem.

%The problem
The MSSC is the most treated formulation in data clustering literature, since it expresses both homogeneity and separation \cite{Hansen2009}. The Euclidean MSSC corresponds to the minimization of the sum-of-squares of Euclidean distances of objects to their cluster means, or equivalently, to the minimization of within-group sum-of-squares \cite{Xavier2011}. In this work we refer to objects or samples to be clustered as data points.

%Although the scope of this work is focused on solving the MSSC problem -- as a first validation of this model-based framework for clustering -- the long-term goal in this research is to have a general framework that solves any clustering model, once the designed meta-heuristic is based on a neighbourhood structure that can be applicable to a wide range of objective functions.

% Contribution
The method proposed in this document is a hybrid genetic algorithm that combines a K-means local improvement procedure with crossover, mutation and diversification operators. This allows to efficiently escape from local minimums and reach high quality solutions. The method outperforms the best current literature results for all considered sets of benchmark instances in terms of solution quality for the MSSC model. In some cases, the results of the K-means algorithm are more than 1100\% off the solution quality of the proposed method. Additionally, as this work is an accurate and stable method for the MSSC objective, it is also an important starting point to solve different objectives in clustering. Although the scope of this work is focused on solving the MSSC problem, the long-term goal in this research is to have a general framework that solves any classical clustering model. As we tackle the clustering problem from an optimization perspective through a meta-heuristic, we can elaborate a neighbourhood structure that is general enough to deal with different models, where the evaluation cost of a solution varies according to the objective. Thus, the proposed method is an important instrument to the coming step of this work, which consists in testing different clustering models and verifying their impact on clustering results.

% Document structure
This document is structured as follows: in Chapter 2, we discuss the differences between the early ad-hoc and the model-based approaches for clustering, focusing in the case of MSSC. In Chapter 3, we describe the proposed methodology, a meta-heuristic designed to solve the MSSC problem. In Chapter 4, we present the experimental results and the analysis we draw from them. Finally, the final remarks and the future perspectives are addressed in Chapter 5.